# DEEP FASHION v2

```sh
# Setup environment
sudo apt install -y virtualenv
sudo apt install -y python-tk

# Tensorflow (optional)
sudo apt-get install python-pip python-dev python-virtualenv # for Python 2.7
virtualenv --system-site-packages tensorflow121_py27_gpu # for Python 2.7
source tensorflow121_py27_gpu/bin/activate
pip install --upgrade tensorflow-gpu  # for Python 2.7 and GPU

pip install -r requirements.txt 

git clone https://gitlab.lrz.de/ga47baz/Deep_Learning.git

# Download vgg16_weights.h5
cd Deep_Learning/DeepFashionV2
mkdir weights
wget https://doc-0g-4s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/cg71u505ib6dhr968vle9k52782tdd19/1501322400000/13951467387256278872/*/0Bz7KyqmuGsilT0J5dmRCM0ROVHc?e=download -O weights/vgg16_weights.h5


mkdir temp
mkdir temp2

```

```sh
# Download DeepFashion Dataset (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/AttributePrediction.html)
# Copy the dataset inside fashion_data. The directory structure should look like:
fashion_data/
---Anno
------list_attr_cloth.txt
------list_attr_img.txt
------list_bbox.txt
------list_category_cloth.txt
------list_category_img.txt
------list_landmarks.txt
---Eval
------list_eval_partition.txt
---Img
------img

```


output	- all the trained weights and bottleneck features are saved here
logs	- logs required to run tensorboard
dataset	- images in this folder will be used for training, validation and testing

```sh
Run one of the 2 scripts below (for categories only):
#   This copies images from fashion_data folder inside dataset/train, dataset/validation and dataset/test
1. python dataset_init_categ.py			

#   This first crops images according to fashion_data/Anno/list_bbox.txt and then copies them inside dataset/train, dataset/validation and dataset/test
2. python dataset_init_categ_crop.py

```

```sh
# This will first create bottleneck features for all images in the dataset and then use them to further train the model top layer. Model is based on VGG16. Output will be trained weights in output folder
python train_multi.py

```

```sh
# This will use the weights(generated by train_multi.py) and finetune the model. Output will be finetuned weights in output folder
python finetuning_multi.py

```

```sh
# This will use the finetuned weights(generated by finetuning_multi.py) to do inference on test set
python test_multi.py
```

##### Misc:
selective_search.py
Working on it. It basically selects ROI(Region of interest) from the input image and passes on to our model for inference. Can use this for bounding box.

Information about dataset categories and coressponding no. of images (generated by dataset_init_categ.py and dataset_init_categ_crop.py):
info_dataset_categ.txt
info_dataset_categ_crop.txt

#### RESULTS

Using 6 categories: Blazer  Jeans  Joggers  Jumpsuit  Leggings  Romper
consisting of 27254 images (cropped) in total. 

All these categories has almost equal no. of images

##### dataset_crop/train
Leggings             : 3571
Joggers              : 3260
Jumpsuit             : 4464
Jeans                : 5126
Blazer               : 5408
Romper               : 5425

##### dataset_crop/validation
Blazer               : 1040
Jumpsuit             : 840
Romper               : 989
Jeans                : 965
Leggings             : 718
Joggers              : 581

##### dataset_crop/test
Joggers              : 575
Jumpsuit             : 849
Romper               : 994
Blazer               : 1047
Jeans                : 985
Leggings             : 724

```sh
# After training
Epoch 16/50
27246/27254 [============================] - ETA: 0s - loss: 0.3269 - acc: 0.8752Epoch 00015: val_loss improved from 0.45614 to 0.45514, saving model to output/best-weights-015-0.3270-0.8752.hdf5
27254/27254 [==============================] - 101s - loss: 0.3270 - acc: 0.8752 - val_loss: 0.4551 - val_acc: 0.8420
```



